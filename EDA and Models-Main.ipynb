{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d0324b20",
   "metadata": {},
   "source": [
    "# Classifying six-figures in malpractice payments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23ae8737",
   "metadata": {},
   "source": [
    "## Imports and taking peek at the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d5c6ed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "from numbers import Number\n",
    "from scipy import stats\n",
    "from matplotlib.colors import ListedColormap\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "import matplotlib as mpl\n",
    "warnings.filterwarnings('ignore')\n",
    "import math\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import KFold, cross_val_score, cross_validate, ShuffleSplit, train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "from sklearn.metrics import classification_report, accuracy_score, ConfusionMatrixDisplay, confusion_matrix\n",
    "from scipy.stats import chi2_contingency\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b66aa7fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('NPDB2401_Modern_Malpractice_Clean.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ca05608",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "731829e9",
   "metadata": {},
   "source": [
    "### Let's see the distrubtion of the payment column "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d80fd5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(data=df, x='PAYMENT')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af6bc240",
   "metadata": {},
   "source": [
    "### Let's scale that distrubtion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0287d220",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(data=df, x='PAYMENT', log_scale=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0ac4a0f",
   "metadata": {},
   "source": [
    "### Should we classify by the digits of the payment?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3f977c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_ref = {}\n",
    "for i in range(2,9):\n",
    "    log_ref[i] = len(df[(df['PAYMENT'] > 10**(i-1)) & (df['PAYMENT'] <= 10**(i))])\n",
    "log_ref "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1887937e",
   "metadata": {},
   "source": [
    "### We should not:\n",
    "   - Massive class imbalace, most payment are 6 figures \n",
    "   - The smaller figuares will be problematic noise\n",
    "### What will do instead:\n",
    "   - Classify payments as under, over, or extacly 6 figures using orinal encoding and model for that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9536491d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['PAYMENT'] = df['PAYMENT'].map(lambda x: math.ceil(math.log(x, 10)) ).astype(int)\n",
    "df['PAYMENT'] = df['PAYMENT'].map(lambda x: 2 if x > 6 else (0 if x < 6 else 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a207cbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_ref = {}\n",
    "for i in range(df['PAYMENT'].min(),df['PAYMENT'].max()+1):\n",
    "    log_ref[i] = len(df[df['PAYMENT'] == i])\n",
    "log_ref "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "644ebd84",
   "metadata": {},
   "source": [
    "### Better, Next Let's look at some numerical correlations to see how each column could affect our models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64273d84",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_col = ['PAYMENT','GRAD','PRACTAGE', 'NPMALRPT', 'NPLICRPT', 'NPCLPRPT' , 'NPPSMRPT' , 'NPDEARPT', 'NPEXCRPT', 'NPGARPT' ,'NPCTMRPT', 'MALTIME', 'NUMBPRSN', 'PTAGE']\n",
    "num_df = df[num_col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bc360f5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 10))\n",
    "sns.heatmap(num_df.corr(), cmap=\"YlGnBu\", annot=True, fmt=\".2f\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7b2ac4f",
   "metadata": {},
   "source": [
    "### The strongest correlations by magnitudes are:\n",
    "   - Grad and Practage\n",
    "   - NPExcrpt and NPMalrpt\n",
    "### Because of that we will remove:\n",
    "   - Practage and NPExcrpt since Grad and NPMalrpt are more correlated to payments\n",
    "   - because of the high correlation Practage and NPExcrpt are redundant "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55afb032",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(df[['PRACTAGE', 'NPEXCRPT']], axis=1, inplace=True)\n",
    "num_col = list(set(num_df.columns) - set(['PRACTAGE', 'NPEXCRPT','PAYMENT']))\n",
    "num_df = df[num_col]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9f9bbd8",
   "metadata": {},
   "source": [
    "### What about the categorical correlations?\n",
    "\n",
    "### Before that we have clean out the irregular categories from our categorical columns with a lot of categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33b1da51",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_cols = list(set(df.columns) - set(num_df.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d842ae99",
   "metadata": {},
   "outputs": [],
   "source": [
    "col_ref = {}\n",
    "for col in cat_cols:\n",
    "    col_ref[col]  = len(df[col].unique())\n",
    "    df[col] = df[col].astype(str)\n",
    "col_ref"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ada21884",
   "metadata": {},
   "source": [
    "### 15 or more catagories counts as categorical column with a lot of categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daf42930",
   "metadata": {},
   "outputs": [],
   "source": [
    "big_cols = [col for col in col_ref.keys() if col_ref[col] > 15]\n",
    "big_cols"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9135645a",
   "metadata": {},
   "source": [
    "### If the catagory appears in less than 1% of the data it will be replaced by a V for Various "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "813c835b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in big_cols:\n",
    "    ref = dict(df[col].value_counts())\n",
    "    df[col] = df[col].map(lambda x: 'V' if ref[x] < 10**-2*len(df) else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "603722a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "col_ref = {}\n",
    "for col in cat_cols:\n",
    "    col_ref[col]  = len(df[col].unique())\n",
    "col_ref"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86998d79",
   "metadata": {},
   "source": [
    "### Better, now for some more quick cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d500a633",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in cat_cols:\n",
    "    if len(df[col].unique()) == 2: \n",
    "        print(f'{col} : {df[col].unique()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a2334fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['PAYNUMBR'] = df['PAYNUMBR'].map(lambda x: '0' if x == 'S' or x == '0' else '1')\n",
    "for col in cat_cols:\n",
    "    if len(df[col].unique()) == 2: \n",
    "        print(f'{col} : {df[col].unique()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af134965",
   "metadata": {},
   "source": [
    "### For categorical correlations we have to use cramers_v,  an effect size measurement for the chi-square test of independence,  as our metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54c8dc2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cramers_v(x, y):\n",
    "    confusion_matrix = pd.crosstab(x, y)\n",
    "    chi2 = chi2_contingency(confusion_matrix)[0]\n",
    "    n = confusion_matrix.sum().sum()\n",
    "    r, k = confusion_matrix.shape\n",
    "    return np.sqrt(chi2 / (n * (min(r, k) - 1)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76d1add9",
   "metadata": {},
   "source": [
    "### Let's get those correlations in a heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85de1bbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_cor_cols = cat_cols\n",
    "corr_matrix = pd.DataFrame(index=cat_cor_cols, columns=cat_cor_cols)\n",
    "for col1 in cat_cor_cols:\n",
    "    for col2 in cat_cor_cols:\n",
    "        if col1 == col2:\n",
    "            corr_matrix.loc[col1, col2] = 1.0\n",
    "        else:\n",
    "            corr_matrix.loc[col1, col2] = cramers_v(df[col1], df[col2])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "313ebc8b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "corr_matrix = corr_matrix.astype(float)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', vmin=0, vmax=1, fmt='.2f')\n",
    "plt.title('Correlation Heatmap for Categorical Variables')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "233e8a8b",
   "metadata": {},
   "source": [
    "### We can see:\n",
    "   - STATEFUND strongly correlates with a handful of the cataories and should be droped\n",
    "   - ISINSURE strongly correlates with a couple of the cataories and should be droped\n",
    "   - LICNSTAT correlates well with WORKSTATE and we have stat if both match, and suits are most likly filed in the same state WORKSTATE, LICNSTAT should be droped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ad8e740",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(df[['STATEFUND', 'LICNSTAT', 'ISINSURE']], axis=1, inplace=True)\n",
    "cat_cols = list(set(cat_cols) - set(['STATEFUND', 'LICNSTAT', 'ISINSURE', 'PAYMENT']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c334d8aa",
   "metadata": {},
   "source": [
    "## Modeling Time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a3b93df",
   "metadata": {},
   "source": [
    "### First training and test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "346672e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df.drop(['PAYMENT'], axis=1) , df['PAYMENT'], test_size=.3, random_state = 31)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "456841e1",
   "metadata": {},
   "source": [
    "### Here an example of how the cataorical columns are encoded, and the columns for both train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fad1a2d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "ohe = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "dummies = ohe.fit_transform(X_train[cat_cols])\n",
    "X_train_onehot = pd.DataFrame(dummies, columns=ohe.get_feature_names_out(), index=X_train.index)\n",
    "X_train_Enc = pd.concat([X_train[num_col], X_train_onehot], axis=1)\n",
    "scaler = StandardScaler()\n",
    "X_train_Enc_ss = scaler.fit_transform(X_train_Enc)\n",
    "len(X_train) == len(X_train_Enc_ss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6fe3a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "dummies = ohe.transform(X_test[cat_cols])\n",
    "X_test_onehot = pd.DataFrame(dummies, columns=ohe.get_feature_names_out(), index=X_test.index)\n",
    "X_test_Enc = pd.concat([X_test[num_col], X_test_onehot], axis=1)\n",
    "X_test_Enc_ss = scaler.transform(X_test_Enc)\n",
    "len(X_test) == len(X_test_Enc_ss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b91f807",
   "metadata": {},
   "source": [
    "### Let's try a simple Decision Tree Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e88293a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DecisionTreeClassifier(criterion = 'gini',  max_depth = 4, random_state=42)\n",
    "model.fit(X_train_Enc_ss, y_train)\n",
    "accuracy_train = model.score(X_train_Enc_ss, y_train)\n",
    "accuracy_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d921677b",
   "metadata": {},
   "source": [
    "### What were the important features, and how important are they"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4130aa01",
   "metadata": {},
   "outputs": [],
   "source": [
    "imp_ref = {X_train_Enc.columns[i]: model.feature_importances_[i] for i in range(len(X_train_Enc.columns)) if model.feature_importances_[i] != 0}\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(imp_ref.keys(), imp_ref.values())\n",
    "plt.xlabel('Features')\n",
    "plt.ylabel('Importance')\n",
    "plt.title('Feature Importances in Decision Tree')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc1a021f",
   "metadata": {},
   "source": [
    "### Let's take a peek into the tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91743dfb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(figsize=(50,30))\n",
    "plot_tree(model, ax=ax, fontsize=10);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2abf33b6",
   "metadata": {},
   "source": [
    "### Is there a risk for variance?\n",
    "\n",
    "### Let's make our own cross_validation method to pervent data leakage and keep our encoding and scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d773de54",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validation(X_train, y_train, model, num_split = 10):\n",
    "        \n",
    "    score_train_list = []\n",
    "    score_val_list = []\n",
    "    \n",
    "    for train_index, valid_index in KFold(n_splits = num_split).split(X_train):\n",
    "        \n",
    "        # train and validation splitting \n",
    "        X_train_fold, X_val_fold = X_train.iloc[train_index], X_train.iloc[valid_index]\n",
    "        y_train_fold, y_val_fold = y_train.iloc[train_index], y_train.iloc[valid_index]\n",
    "        \n",
    "        ohe = OneHotEncoder(sparse_output=False)\n",
    "        dummies = ohe.fit_transform(X_train_fold[cat_cols])\n",
    "        X_train_onehot = pd.DataFrame(dummies, columns=ohe.get_feature_names_out(), index=X_train_fold.index)\n",
    "        X_train_Enc = pd.concat([X_train_fold[num_col], X_train_onehot], axis=1)\n",
    "        \n",
    "        dummies = ohe.transform(X_val_fold[cat_cols])\n",
    "        X_val_onehot = pd.DataFrame(dummies, columns=ohe.get_feature_names_out(), index=X_val_fold.index)\n",
    "        X_val_Enc = pd.concat([X_val_fold[num_col], X_val_onehot], axis=1)\n",
    "\n",
    "        #create/fit the Standard scaler on the train fold\n",
    "        scaler = StandardScaler()\n",
    "        X_tf_sc = scaler.fit_transform(X_train_Enc)\n",
    "        # transform validation fold\n",
    "        X_vld_sc = scaler.transform(X_val_Enc)\n",
    "\n",
    "        # Score it\n",
    "        model.fit(X_tf_sc, y_train_fold)\n",
    "        \n",
    "        # now how did we do?\n",
    "        accuracy_train = model.score(X_tf_sc, y_train_fold)\n",
    "        accuracy_val = accuracy_score(y_val_fold, model.predict(X_vld_sc))\n",
    "        score_val_list.append(accuracy_val)\n",
    "        score_train_list.append(accuracy_train)\n",
    "    \n",
    "    return {'train': np.mean(score_train_list), 'validation': np.mean(score_val_list)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2992aa68",
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_validation(X_train, y_train, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b161256",
   "metadata": {},
   "source": [
    "### There does not seem to be. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42afccfb",
   "metadata": {},
   "source": [
    "### Let's try uping the max_depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16954bfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "g_model = DecisionTreeClassifier(criterion = 'gini',  max_depth = 7, random_state=42)\n",
    "g_model.fit(X_train_Enc_ss, y_train)\n",
    "accuracy_train = g_model.score(X_train_Enc_ss, y_train)\n",
    "accuracy_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cdece14",
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_validation(X_train, y_train, g_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "831ee3df",
   "metadata": {},
   "source": [
    "### A seeming minmal risk of variance\n",
    "### Important features?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26de0d15",
   "metadata": {},
   "outputs": [],
   "source": [
    "imp_ref = {X_train_Enc.columns[i]: g_model.feature_importances_[i] for i in range(len(X_train_Enc.columns)) if g_model.feature_importances_[i] > .01}\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(imp_ref.keys(), imp_ref.values())\n",
    "plt.xlabel('Features')\n",
    "plt.ylabel('Importance')\n",
    "plt.title('Feature Importances in Decision Tree (gini)')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94e1a901",
   "metadata": {},
   "source": [
    "### Same model but with a entropy criterion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dab7913f",
   "metadata": {},
   "outputs": [],
   "source": [
    "e_model = DecisionTreeClassifier(criterion = 'entropy',  max_depth = 7, random_state=42)\n",
    "e_model.fit(X_train_Enc_ss, y_train)\n",
    "accuracy_train = e_model.score(X_train_Enc_ss, y_train)\n",
    "accuracy_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f01919b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_validation(X_train, y_train, e_model, num_split = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "653dd064",
   "metadata": {},
   "source": [
    "### A seeming minmal risk of variance\n",
    "### Important features?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d05e1d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "imp_ref = {X_train_Enc.columns[i]: e_model.feature_importances_[i] for i in range(len(X_train_Enc.columns)) if e_model.feature_importances_[i] > .01}\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(imp_ref.keys(), imp_ref.values())\n",
    "plt.xlabel('Features')\n",
    "plt.ylabel('Importance')\n",
    "plt.title('Feature Importances in Decision Tree (entropy)')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9372beb",
   "metadata": {},
   "source": [
    "### Let's see the final results for the entropy criterion model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d79e1532",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(y_test, e_model.predict(X_test_Enc_ss))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e0f260f",
   "metadata": {},
   "source": [
    "### In more detail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a62171a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, e_model.predict(X_test_Enc_ss)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff9f72d0",
   "metadata": {},
   "source": [
    "### In visual detail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30d15400",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfmat = confusion_matrix(y_test, e_model.predict(X_test_Enc_ss))\n",
    "\n",
    "ConfusionMatrixDisplay(cfmat,display_labels=['Under', 'Right', 'Over']).plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7df63658",
   "metadata": {},
   "source": [
    "### Let's see the final results for the gini criterion model with max_depth 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c613f13",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(y_test, g_model.predict(X_test_Enc_ss))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7634c43b",
   "metadata": {},
   "source": [
    "### In more detail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e959070a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, g_model.predict(X_test_Enc_ss)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b4d398c",
   "metadata": {},
   "source": [
    "### In visual detail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f16707f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "cfmat = confusion_matrix(y_test, g_model.predict(X_test_Enc_ss))\n",
    "\n",
    "ConfusionMatrixDisplay(cfmat,display_labels=['Under', 'Right', 'Over']).plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbe5d9a4",
   "metadata": {},
   "source": [
    "### Lastly, our oringal base tree model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74438352",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(y_test, model.predict(X_test_Enc_ss))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73aa627a",
   "metadata": {},
   "source": [
    "### In more detail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6abe2b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, model.predict(X_test_Enc_ss)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0890594",
   "metadata": {},
   "source": [
    "### In visual detail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2a2133a",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfmat = confusion_matrix(y_test, model.predict(X_test_Enc_ss))\n",
    "\n",
    "ConfusionMatrixDisplay(cfmat,display_labels=['Under', 'Right', 'Over']).plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aad12c8",
   "metadata": {},
   "source": [
    "## Decision Tree Classifier Summary:\n",
    "\n",
    "### Best model is gini criterion model with max_depth 7 by accuracy\n",
    "\n",
    "### We can tell what is important, but not how impactful it for the payment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d41bb57",
   "metadata": {},
   "source": [
    "## Let's try Logistical Regression\n",
    "### We can find out only what is important, but how impactful it for the payment.\n",
    "### Starting with a basic model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30ec5dbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_log_basic = LogisticRegression()\n",
    "model_log_basic.fit(X_train_Enc_ss, y_train)\n",
    "model_log_basic.score(X_train_Enc_ss, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cef4e33d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_validation(X_train, y_train, model_log_basic, num_split = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f541a047",
   "metadata": {},
   "source": [
    "### A seeming minmal risk of variance\n",
    "### Important weights?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "047e6f05",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows=3, ncols=1, figsize=(10, 12))\n",
    "cat = ['Under', 'Exactly', 'Over']\n",
    "thersh = [.15, .03, .15]\n",
    "for i in range(3):\n",
    "    imp_ref = {X_train_Enc.columns[k]: model_log_basic.coef_[i][k] for k in range(len(X_train_Enc.columns)) if np.abs(model_log_basic.coef_[i][k]) >= thersh[i]}\n",
    "    sns.barplot(y=imp_ref.keys(), x=imp_ref.values(), orient='h', ax=axes[i])\n",
    "    axes[i].set_xlabel('Importance')\n",
    "    axes[i].set_ylabel('Features')\n",
    "    axes[i].set_title(f'Weights for {cat[i]} six-figures in Basic Logistic Regression Model')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bd5446b",
   "metadata": {},
   "source": [
    "### Let's try optimizing \n",
    "### What is the best C?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13580b9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_hyper_opt(X_train, y_train):\n",
    "    C_list = [1e-4,1e-3, 1e-2, 1e-1, 1, 10, 100, 1e3]\n",
    "    cv_scores = {}\n",
    "    for c in C_list:\n",
    "        logreg = LogisticRegression(C = c)\n",
    "        cv_loop_results = cross_validation(X_train, y_train, logreg, num_split = 10)\n",
    "        cv_scores[c] = cv_loop_results['validation']\n",
    "    return cv_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb0dc058",
   "metadata": {},
   "source": [
    "### WARING this script is UBER SLOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c8a1205",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_hyper_opt(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c83351b",
   "metadata": {},
   "source": [
    "### C = 10 is  the best "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82a02ab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_log_c10 = LogisticRegression(C=10)\n",
    "model_log_c10.fit(X_train_Enc_ss, y_train)\n",
    "model_log_c10.score(X_train_Enc_ss, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5867c09d",
   "metadata": {},
   "source": [
    "### A seeming minmal risk of variance\n",
    "### Important weights?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43655f84",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows=3, ncols=1, figsize=(10, 12))\n",
    "cat = ['Under', 'Exactly', 'Over']\n",
    "thersh = [.15, .03, .15]\n",
    "for i in range(3):\n",
    "    imp_ref = {X_train_Enc.columns[k]: model_log_c10.coef_[i][k] for k in range(len(X_train_Enc.columns)) if np.abs(model_log_c10.coef_[i][k]) >= thersh[i]}\n",
    "    sns.barplot(y=imp_ref.keys(), x=imp_ref.values(), orient='h', ax=axes[i])\n",
    "    axes[i].set_xlabel('Importance')\n",
    "    axes[i].set_ylabel('Features')\n",
    "    axes[i].set_title(f'Weights for {cat[i]} six-figures in Basic Logistic Regression Model with Optimal Regularization')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75f71b7d",
   "metadata": {},
   "source": [
    "### Is there a better solver?\n",
    "### WARING this script is UBER SLOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57929e6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "solver_ref = {}.fromkeys(['newton-cg', 'sag', 'saga', 'lbfgs']) \n",
    "for s in solver_ref.keys():\n",
    "    model_log_c10_slove = LogisticRegression(C=10, solver=s)\n",
    "    model_log_c10_slove.fit(X_train_Enc_ss, y_train)\n",
    "    solver_ref[s] = model_log_c10_slove.score(X_train_Enc_ss, y_train)\n",
    "solver_ref"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "799ba29a",
   "metadata": {},
   "source": [
    "### Newton-cg is the best "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50edc8c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_log_c10_newton = LogisticRegression(C=10, solver='newton-cg')\n",
    "model_log_c10_newton.fit(X_train_Enc_ss, y_train)\n",
    "cross_validation(X_train, y_train, model_log_c10_newton, num_split = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2de48c93",
   "metadata": {},
   "source": [
    "### A seeming minmal risk of variance\n",
    "### Important weights?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d7a4e8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows=3, ncols=1, figsize=(10, 12))\n",
    "cat = ['Under', 'Exactly', 'Over']\n",
    "thersh = [.15, .03, .15]\n",
    "for i in range(3):\n",
    "    imp_ref = {X_train_Enc.columns[k]: model_log_c10.coef_[i][k] for k in range(len(X_train_Enc.columns)) if np.abs(model_log_c10.coef_[i][k]) >= thersh[i]}\n",
    "    sns.barplot(y=imp_ref.keys(), x=imp_ref.values(), orient='h', ax=axes[i])\n",
    "    axes[i].set_xlabel('Importance')\n",
    "    axes[i].set_ylabel('Features')\n",
    "    axes[i].set_title(f'Weights for {cat[i]} six-figures in Basic Logistic Regression Model with Optimal Regularization')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0290c29e",
   "metadata": {},
   "source": [
    "## Let's see the final results for the Logistical Regression models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0236f93",
   "metadata": {},
   "source": [
    "### Basic Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20407d3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(y_test, model_log_basic.predict(X_test_Enc_ss))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05bd67a6",
   "metadata": {},
   "source": [
    "### In more detail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7bda1c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, model_log_basic.predict(X_test_Enc_ss)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baab33d1",
   "metadata": {},
   "source": [
    "### In visual detail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "037a1f57",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfmat = confusion_matrix(y_test, model_log_basic.predict(X_test_Enc_ss))\n",
    "\n",
    "ConfusionMatrixDisplay(cfmat,display_labels=['Under', 'Right', 'Over']).plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d9ece3e",
   "metadata": {},
   "source": [
    "### Default Solver with C=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88283d80",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(y_test, model_log_c10.predict(X_test_Enc_ss))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c859c87",
   "metadata": {},
   "source": [
    "### In more detail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76933f99",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, model_log_c10.predict(X_test_Enc_ss)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70b876f8",
   "metadata": {},
   "source": [
    "### In visual detail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37ec839e",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfmat = confusion_matrix(y_test, model_log_c10.predict(X_test_Enc_ss))\n",
    "\n",
    "ConfusionMatrixDisplay(cfmat,display_labels=['Under', 'Right', 'Over']).plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4a9197d",
   "metadata": {},
   "source": [
    "### Solver with newton-cg with C=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d8ff6ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(y_test, model_log_c10_newton.predict(X_test_Enc_ss))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b40d9b88",
   "metadata": {},
   "source": [
    "### In more detail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01a2b970",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, model_log_c10_newton.predict(X_test_Enc_ss)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20bc5caf",
   "metadata": {},
   "source": [
    "### In visual detail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3886c05f",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfmat = confusion_matrix(y_test, model_log_c10.predict(X_test_Enc_ss))\n",
    "\n",
    "ConfusionMatrixDisplay(cfmat,display_labels=['Under', 'Right', 'Over']).plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3017fd8d",
   "metadata": {},
   "source": [
    "## Logistic Regression Summary:\n",
    "   \n",
    "### They all preforms similarly but C=10 and lbfgs solver preforms the best\n",
    "\n",
    "### Important notes\n",
    "   - The higher the outcome, the more severe the injury to the patient (1 to 9 only)\n",
    "   - Where the practitioner is working matters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94543893",
   "metadata": {},
   "source": [
    "# Summary\n",
    "\n",
    "### Best model is a C=10 and lbfgs solver logistical regression and it call t\n",
    "\n",
    "## Recomendtions:\n",
    "   - If you are a malpratice insurce company look into spending more in advertising in New York, and illinois. According to our model thoes states tend to a have higher than a six-figures malpratice settlements. You can pull funding from advertising in Puerto Rico and to advertising to dentist, since those payments tend to be smaller than six-figures according to the model. \n",
    "\n",
    "   - As lawyer, as you may expect, outcome is the most important factor it worth focusing on that when coming to a decision of giving a six-figure settlment or something higher or lower than that. The more server the higher the outcome (1-9).\n",
    "   \n",
    "## What is next\n",
    "   - An EDA to fine tune the recomendtions.\n",
    "   - Maybe try a regression models, but use the classifier methods since the numerical data is encoded,for example PRACTAGE, PTAGE does not give the age of the practitioner or patient respectively rather an age group they belong to. On top of that over half of the data is categorical making hard of tradional regression to work properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96252003",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (learn-env)",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
